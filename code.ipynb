{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2301f9d-8a2a-434d-83c1-5a829170d32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9175d5c3-5b29-4827-bfc1-9f8030d31618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=c53185c44c3915f44143531117db5d5b0c6bdda5166a86a3286cd1efa610b240\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: nltk, absl-py, rouge_score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [rouge_score]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 nltk-3.9.2 rouge_score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645cdad2-ed7e-4237-83fb-2f9dbfbb00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"prompt_data.jsonl\"\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUT_DIR = \"./prompt_lora_ckpt\"\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "MAX_NEW_TOKENS = 80\n",
    "EPOCHS = 3\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4c00a0-c4e1-400e-b869-e0686c6f5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: CONFIG\n",
      "{'DATA_PATH': 'prompt_data.jsonl', 'BASE_MODEL_NAME': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'OUT_DIR': './prompt_lora_ckpt', 'SEED': 42, 'MAX_LEN': 256, 'MAX_NEW_TOKENS': 80, 'EPOCHS': 3}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"CONFIG\", {\n",
    "    \"DATA_PATH\": DATA_PATH,\n",
    "    \"BASE_MODEL_NAME\": BASE_MODEL_NAME,\n",
    "    \"OUT_DIR\": OUT_DIR,\n",
    "    \"SEED\": SEED,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"MAX_NEW_TOKENS\": MAX_NEW_TOKENS,\n",
    "    \"EPOCHS\": EPOCHS\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9cb0930-2174-4f84-b194-7a808e482983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Total samples\n",
      "61\n",
      "======================================================================\n",
      "\n",
      " DEBUG: Sample[0] raw\n",
      "{'input': 'Make a prompt that summarizes an academic paper for a conference reviewer.', 'output': 'Summarize the paper. Use 3 bullet points only. Make sure it is under 150 words.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "debug(\"Total samples\", len(ds))\n",
    "debug(\"Sample[0] raw\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2813312d-108c-4623-aa69-c50994bedade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: After shuffle sample[0]\n",
      "{'input': 'Make a prompt that summarizes a technical blog post.', 'output': 'Summarize the blog. Use 3 bullet points only. Make sure to keep it under 100 words.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "ds = ds.shuffle(seed=SEED)\n",
    "debug(\"After shuffle sample[0]\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d7388c-98bd-410c-842e-5bbfa1c2da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Train size\n",
      "51\n",
      "======================================================================\n",
      "\n",
      " DEBUG: Test size\n",
      "10\n",
      "======================================================================\n",
      "\n",
      " DEBUG: test sample\n",
      "{'input': 'Make a prompt that checks if a sentence is too long.', 'output': 'Check the sentence length. Explain if it is too long. Make sure to rewrite it shorter.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Train size\", len(train_ds_raw))\n",
    "debug(\"Test size\", len(test_ds_raw))\n",
    "debug(\"test sample\", test_ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e137b5-9ef7-4caa-800e-abc02c9d4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a4c4ff-89fe-4bd8-a214-d60e51e217bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e5c97d3-9f4b-4b23-84ba-5d6335e0f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(inp: str) -> str:\n",
    "    return f\"### Instruction:\\n{inp}\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ee37d60-b13c-4afa-84b4-d4b0a8efdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    inp = example[\"input\"]\n",
    "    out = example[\"output\"]\n",
    "\n",
    "    prompt = format_prompt(inp)\n",
    "    full_text = prompt + out\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=MAX_LEN)[\"input_ids\"]\n",
    "\n",
    "    full_enc = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    input_ids = full_enc[\"input_ids\"]\n",
    "    attention_mask = full_enc[\"attention_mask\"]\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "    prompt_len = min(len(prompt_ids), len(input_ids))\n",
    "    labels[prompt_len:] = input_ids[prompt_len:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761abd23-51e9-4e7e-ab73-c2df48ddae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = preprocess(train_ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1708eccf-9056-4e32-aefc-d1ccceb75912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Formatted prompt example\n",
      "### Instruction:\n",
      "Make a prompt that explains a machine learning concept in simple words.\n",
      "\n",
      "### Response:\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Formatted prompt example\", format_prompt(train_ds_raw[0][\"input\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e9c0a10-a751-43e6-83d8-a4137e148bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Full text example\n",
      "### Instruction:\n",
      "Make a prompt that explains a machine learning concept in simple words.\n",
      "\n",
      "### Response:\n",
      "Explain the concept in simple words. Make sure it is under 120 words. Keep it clear.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Full text example\", format_prompt(train_ds_raw[0][\"input\"]) + train_ds_raw[0][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58571cde-68d4-4964-9bc4-0b5d6ca14416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: input_ids[:40]\n",
      "[1, 835, 2799, 4080, 29901, 13, 9984, 263, 9508, 393, 18568, 263, 4933, 6509, 6964, 297, 2560, 3838, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 9544, 7420, 278, 6964, 297, 2560, 3838, 29889, 8561, 1854, 372, 338, 1090, 29871]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"input_ids[:40]\", tmp[\"input_ids\"][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43cf4813-84a7-4f18-a925-182d3dd717e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: labels[:40]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9544, 7420, 278, 6964, 297, 2560, 3838, 29889, 8561, 1854, 372, 338, 1090, 29871]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"labels[:40]\", tmp[\"labels\"][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f791bbb-8ac4-4d20-808e-e02c6559e441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089d6c2fce814a94be30b82d7a21b2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8476d64384945238c706bbf4e74ac93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds_raw.map(preprocess, remove_columns=train_ds_raw.column_names)\n",
    "test_ds = test_ds_raw.map(preprocess, remove_columns=test_ds_raw.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bf3c003-e6a6-4c75-ad2a-80367b32d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausalLMWithLabels:\n",
    "    tokenizer: AutoTokenizer\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "     \n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        for f in features:\n",
    "            f.pop(\"labels\")\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,          # pad to longest\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded_labels = []\n",
    "        for lab in labels:\n",
    "            pad_len = max_len - len(lab)\n",
    "            if pad_len > 0:\n",
    "                lab = lab + [self.label_pad_token_id] * pad_len\n",
    "            else:\n",
    "                lab = lab[:max_len]  # safety trim\n",
    "            padded_labels.append(lab)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9edd2cca-9c34-4dcb-93af-f61a46254bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0be1fea2-870e-4e7e-b5d7-9595b5235f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d20d5839-66b3-44c5-809c-626f144ea27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e1c7b22-8b65-4301-975d-dfded4bdcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1828/1138148192.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCausalLMWithLabels(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea4b5149-4b19-4bd2-8c72-f6688dacc82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.872300</td>\n",
       "      <td>1.955878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.005700</td>\n",
       "      <td>1.514791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.282800</td>\n",
       "      <td>1.425319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=1.8826042129879905, metrics={'train_runtime': 31.8787, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.659, 'total_flos': 47542586191872.0, 'train_loss': 1.8826042129879905, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f3c6a39-fdc4-42c5-89ef-3000fc27374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./prompt_lora_ckpt/tokenizer_config.json',\n",
       " './prompt_lora_ckpt/special_tokens_map.json',\n",
       " './prompt_lora_ckpt/chat_template.jinja',\n",
       " './prompt_lora_ckpt/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdcd50d9-18c8-4032-b13d-810f65b3308f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "vanilla_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af04f4d3-1f04-43ab-b466-ab5c8a8b7c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_base = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "trained_model = PeftModel.from_pretrained(trained_base, OUT_DIR)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4515c847-0564-4393-ac81-850dd2133cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_for_input(model, inp: str):\n",
    "    prompt = format_prompt(inp)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in decoded:\n",
    "        decoded = decoded.split(\"### Response:\")[-1].strip()\n",
    "    return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8184dfa8-a372-496b-bfb9-1ab321481564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test 1]\n",
      "INPUT: Make a prompt that checks if a sentence is too long.\n",
      "GOLD: Check the sentence length. Explain if it is too long. Make sure to rewrite it shorter.\n",
      "VANILLA: ```\n",
      "Please enter a sentence:\n",
      "I love you more than anything in this world.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "The sentence you entered is too long. Please enter a sentence that is less than 100 characters long. ```\n",
      "\n",
      "### Explanation:\n",
      "The prompt checks if the entered sentence is longer than 100 characters. If it\n",
      "TRAINED: Check if the sentence is too long. Explain why. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 2]\n",
      "INPUT: Make a prompt that summarizes a model architecture.\n",
      "GOLD: Summarize the architecture. Use bullet points only. Make sure to highlight key steps.\n",
      "VANILLA: Here's a prompt that summarizes a model architecture:\n",
      "\n",
      "```\n",
      "Enter the name of the model architecture: ResNet50\n",
      "```\n",
      "\n",
      "This prompt asks the user to enter the name of the model architecture, which is ResNet50.\n",
      "TRAINED: Summarize the model architecture. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 3]\n",
      "INPUT: Make a prompt that extracts cause and effect relations.\n",
      "GOLD: Extract the cause and the effect from the text. Make sure to keep them short.\n",
      "VANILLA: Please provide a prompt that extracts cause and effect relations. Here's an example:\n",
      "\n",
      "```\n",
      "Please provide a prompt that extracts cause and effect relations:\n",
      "\n",
      "1. What is the cause of the increase in traffic on the website?\n",
      "2. What is the effect of the increase in traffic on the website's sales?\n",
      "3. What is the cause of the increase\n",
      "TRAINED: Extract cause and effect relations. Make sure to use bullet points. Make sure to use clear bullet points. Make sure to make sure to use bullet points. Make sure to make sure to use bullet points. Make sure to make sure to use bullet points. Make sure to make sure to use bullet points. Make sure to make sure to use bullet points. Make sure to make sure to use bullet\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 4]\n",
      "INPUT: Make a prompt that analyzes the tone of a message.\n",
      "GOLD: Analyze the tone. Explain your judgement in one sentence. Make sure the answer is short.\n",
      "VANILLA: ```\n",
      "Please provide me with a prompt that analyzes the tone of a message.\n",
      "\n",
      "Response:\n",
      "Please provide me with a prompt that analyzes the tone of a message.\n",
      "\n",
      "Prompt:\n",
      "Please provide me with a prompt that analyzes the tone of a message.\n",
      "\n",
      "Response:\n",
      "Please provide me with a prompt that analyzes the tone of a message.\n",
      "TRAINED: Analyze the tone. Explain why. Make sure to use specific examples. Make sure to use a clear and concise sentence. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proofread. Make sure to proof\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 5]\n",
      "INPUT: Make a prompt that rewrites unclear experiment setup text.\n",
      "GOLD: Rewrite the setup more clearly. Make sure to keep the details and shorten the sentences.\n",
      "VANILLA: ```\n",
      "Please provide a clear and concise prompt that rewrites the experiment setup text in a way that is easy to understand. The prompt should include the experiment type, the variables being measured, and any relevant information about the experiment. The prompt should also include any necessary instructions or guidelines for the participant to follow.\n",
      "TRAINED: Rewrite the text in clear and concise sentences. Make sure to keep the same structure. Make sure to use bullet points. Make sure to use short sentences. Make sure to use active voice. Make sure to use short sentences. Make sure to use active voice. Make sure to use short sentences. Make sure to use active voice. Make sure to use short sentences. Make sure to use active\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 6]\n",
      "INPUT: Make a prompt that summarizes a long article for a casual reader.\n",
      "GOLD: Summarize the article. Use 3 short bullet points only. Make sure the tone is casual and clear.\n",
      "VANILLA: Here's a summary of the article for a casual reader:\n",
      "\n",
      "The article discusses the latest advancements in renewable energy technology, including solar panels, wind turbines, and hydroelectric power. It highlights the benefits of these technologies, such as reducing carbon emissions and improving energy security. The article also provides examples of successful implementations of these techn\n",
      "TRAINED: Summarize the article in 10 bullet points. Make sure to keep it short. Make sure to use bullet points. Make sure to use short bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use bullet points. Make sure to use\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 7]\n",
      "INPUT: Make a prompt that converts raw text into bullet points.\n",
      "GOLD: Convert the text into 4 bullet points. Make sure each point is under 15 words.\n",
      "VANILLA: ```\n",
      "Please enter a piece of text: This is a test text.\n",
      "\n",
      "Here's how it would look like with bullet points:\n",
      "\n",
      "1. This is the first bullet point.\n",
      "2. This is the second bullet point.\n",
      "3. This is the third bullet point.\n",
      "\n",
      "Note: The text will be converted to bullet points based on the number of spaces between the\n",
      "TRAINED: Make bullet points. Make sure they are short. Make sure they are clear. Make sure they are easy to read. Make sure they are short. Make sure they are clear. Make sure they are easy to read. Make sure they are short. Make sure they are clear. Make sure they are easy to read. Make sure they are short. Make sure they are clear. Make sure they are\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 8]\n",
      "INPUT: Make a prompt that extracts structured info from a messy paragraph.\n",
      "GOLD: Extract the info from the text. Make sure to output it as JSON. Keep the fields simple and clear.\n",
      "VANILLA: ```\n",
      "Please enter a messy paragraph:\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed euismod, nulla sit amet bibendum commodo, nulla nulla bibendum nunc, eu ultricies nulla nulla sit amet nisl. Sed euismod, nulla sit amet bibendum commodo,\n",
      "TRAINED: Extract structured info. Make sure it is clear and concise. Make sure it is not too long. Make sure it is not too short. Make sure it is not too confusing. Make sure it is not too short. Make sure it is not too long. Make sure it is not too short. Make sure it is not too long. Make sure it is not too short. Make\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 9]\n",
      "INPUT: Make a prompt that rewrites Korean text in a more concise tone.\n",
      "GOLD: Rewrite the Korean text in a concise tone. Make sure the meaning stays the same. Show only the revised text.\n",
      "VANILLA: 잘 못해요. 저는 잘 못해요. 저는 잘 못해요. 저는 잘 못해요. 저는 잘 못해요. 저는\n",
      "TRAINED: Rewrite the text in a more concise tone. Make sure it still makes sense. Make sure to use fewer words. Make sure it's clear. Make sure it's short. Make sure it's short. Make sure it's short. Make sure it's short. Make sure it's short. Make sure it's short. Make sure it's short\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 10]\n",
      "INPUT: Make a prompt that summarizes an experimental table with mean ± std.\n",
      "GOLD: Summarize the table. Show mean and std only. Make sure to bold the best scores and explain the trend in 3 sentences.\n",
      "VANILLA: Please enter the name of the table you want to generate:\n",
      "\n",
      "### Instruction:\n",
      "Generate a table with the mean and standard deviation of the data.\n",
      "\n",
      "###\n",
      "TRAINED: Summarize the table. Make sure to include the mean and standard deviation. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make sure to use short sentences. Make sure to use bullet points. Make\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, ex in enumerate(test_ds_raw):\n",
    "    inp = ex[\"input\"]\n",
    "    gold = ex[\"output\"]\n",
    "\n",
    "    vanilla_pred = generate_for_input(vanilla_model, inp)\n",
    "    trained_pred = generate_for_input(trained_model, inp)\n",
    "\n",
    "    results.append({\n",
    "        \"input\": inp,\n",
    "        \"gold\": gold,\n",
    "        \"vanilla_pred\": vanilla_pred,\n",
    "        \"trained_pred\": trained_pred\n",
    "    })\n",
    "\n",
    "    print(f\"\\n[Test {i+1}]\")\n",
    "    print(\"INPUT:\", inp)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"VANILLA:\", vanilla_pred)\n",
    "    print(\"TRAINED:\", trained_pred)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a93cb10b-d140-4500-b29f-0256a96ff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer_R = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64ac1199-841a-4723-85c4-5bf3f7138240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0c8192a-bc43-4b9e-a3b5-db39ff7b3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rouge_f1(pred, ref):\n",
    "    s = scorer.score(ref, pred)\n",
    "    return s[\"rouge1\"].fmeasure, s[\"rougeL\"].fmeasure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "791bb0ae-0ef0-4a15-b226-5785bc5cccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OVERALL ROUGE (mean over test set) =====\n",
      "Vanilla  ROUGE-1: 0.1562\n",
      "Vanilla  ROUGE-L: 0.1347\n",
      "Trained  ROUGE-1: 0.1990\n",
      "Trained  ROUGE-L: 0.1872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "van_r1s, van_rls = [], []\n",
    "trn_r1s, trn_rls = [], []\n",
    "\n",
    "for r in results:\n",
    "    vr1, vrl = rouge_f1(r[\"vanilla_pred\"], r[\"gold\"])\n",
    "    tr1, trl = rouge_f1(r[\"trained_pred\"], r[\"gold\"])\n",
    "\n",
    "    van_r1s.append(vr1); van_rls.append(vrl)\n",
    "    trn_r1s.append(tr1); trn_rls.append(trl)\n",
    "\n",
    "# overall mean (you can also print std if you want)\n",
    "print(\"\\n===== OVERALL ROUGE (mean over test set) =====\")\n",
    "print(f\"Vanilla  ROUGE-1: {np.mean(van_r1s):.4f}\")\n",
    "print(f\"Vanilla  ROUGE-L: {np.mean(van_rls):.4f}\")\n",
    "print(f\"Trained  ROUGE-1: {np.mean(trn_r1s):.4f}\")\n",
    "print(f\"Trained  ROUGE-L: {np.mean(trn_rls):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f33e6a48-050f-4883-997a-fdcbe91d1186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Saved file\n",
      "test_generations_with_rouge.jsonl\n",
      "======================================================================\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_generations_with_rouge.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, r in enumerate(results):\n",
    "        r[\"vanilla_rouge1\"] = van_r1s[i]\n",
    "        r[\"vanilla_rougeL\"] = van_rls[i]\n",
    "        r[\"trained_rouge1\"] = trn_r1s[i]\n",
    "        r[\"trained_rougeL\"] = trn_rls[i]\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "debug(\"Saved file\", \"test_generations_with_rouge.jsonl\")\n",
    "print(\"DONE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
