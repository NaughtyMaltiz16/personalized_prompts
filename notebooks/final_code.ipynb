{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2301f9d-8a2a-434d-83c1-5a829170d32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9175d5c3-5b29-4827-bfc1-9f8030d31618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d857b4ce-6558-473a-8a2c-f5bd84215603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645cdad2-ed7e-4237-83fb-2f9dbfbb00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"prompt_data.jsonl\"\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUT_DIR = \"./prompt_lora_ckpt\"\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "MAX_NEW_TOKENS = 80\n",
    "EPOCHS = 3\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737cdf1b-3e07-44f3-bfcf-6a62aa96f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(title, value=None):\n",
    "    print(f\"\\n DEBUG: {title}\")\n",
    "    if value is not None:\n",
    "        print(value)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4c00a0-c4e1-400e-b869-e0686c6f5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: CONFIG\n",
      "{'DATA_PATH': 'prompt_data.jsonl', 'BASE_MODEL_NAME': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'OUT_DIR': './prompt_lora_ckpt', 'SEED': 42, 'MAX_LEN': 256, 'MAX_NEW_TOKENS': 80, 'EPOCHS': 3}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"CONFIG\", {\n",
    "    \"DATA_PATH\": DATA_PATH,\n",
    "    \"BASE_MODEL_NAME\": BASE_MODEL_NAME,\n",
    "    \"OUT_DIR\": OUT_DIR,\n",
    "    \"SEED\": SEED,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"MAX_NEW_TOKENS\": MAX_NEW_TOKENS,\n",
    "    \"EPOCHS\": EPOCHS\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9cb0930-2174-4f84-b194-7a808e482983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Total samples\n",
      "61\n",
      "======================================================================\n",
      "\n",
      " DEBUG: Sample[0] raw\n",
      "{'input': 'Make a prompt that summarizes an academic paper for a conference reviewer.', 'output': 'Summarize the paper. Use 3 bullet points only. Make sure it is under 150 words.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "debug(\"Total samples\", len(ds))\n",
    "debug(\"Sample[0] raw\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2813312d-108c-4623-aa69-c50994bedade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: After shuffle sample[0]\n",
      "{'input': 'Make a prompt that checks if a sentence is too long.', 'output': 'Check the sentence length. Explain if it is too long. Make sure to rewrite it shorter.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "ds = ds.shuffle(seed=SEED)\n",
    "debug(\"After shuffle sample[0]\", ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf29da3-5a9a-4cd2-affd-46eb9f64b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "split = ds.train_test_split(test_size=test_size, seed=SEED)\n",
    "train_ds_raw = split[\"train\"]\n",
    "test_ds_raw = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d7388c-98bd-410c-842e-5bbfa1c2da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Train size\n",
      "51\n",
      "======================================================================\n",
      "\n",
      " DEBUG: Test size\n",
      "10\n",
      "======================================================================\n",
      "\n",
      " DEBUG: test sample\n",
      "{'input': 'Make a prompt that extracts pros and cons.', 'output': 'Extract pros and cons. Make sure to output two bullet lists: Pros and Cons.'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Train size\", len(train_ds_raw))\n",
    "debug(\"Test size\", len(test_ds_raw))\n",
    "debug(\"test sample\", test_ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e137b5-9ef7-4caa-800e-abc02c9d4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a4c4ff-89fe-4bd8-a214-d60e51e217bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5c97d3-9f4b-4b23-84ba-5d6335e0f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(inp: str) -> str:\n",
    "    return f\"### Instruction:\\n{inp}\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee37d60-b13c-4afa-84b4-d4b0a8efdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    inp = example[\"input\"]\n",
    "    out = example[\"output\"]\n",
    "\n",
    "    prompt = format_prompt(inp)\n",
    "    full_text = prompt + out\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=MAX_LEN)[\"input_ids\"]\n",
    "\n",
    "    full_enc = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    input_ids = full_enc[\"input_ids\"]\n",
    "    attention_mask = full_enc[\"attention_mask\"]\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "    prompt_len = min(len(prompt_ids), len(input_ids))\n",
    "    labels[prompt_len:] = input_ids[prompt_len:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "761abd23-51e9-4e7e-ab73-c2df48ddae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = preprocess(train_ds_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1708eccf-9056-4e32-aefc-d1ccceb75912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Formatted prompt example\n",
      "### Instruction:\n",
      "Make a prompt that checks if a summary kept the main idea.\n",
      "\n",
      "### Response:\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Formatted prompt example\", format_prompt(train_ds_raw[0][\"input\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9c0a10-a751-43e6-83d8-a4137e148bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Full text example\n",
      "### Instruction:\n",
      "Make a prompt that checks if a summary kept the main idea.\n",
      "\n",
      "### Response:\n",
      "Check if the summary kept the main idea. Explain your judgement shortly.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"Full text example\", format_prompt(train_ds_raw[0][\"input\"]) + train_ds_raw[0][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58571cde-68d4-4964-9bc4-0b5d6ca14416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: input_ids[:40]\n",
      "[1, 835, 2799, 4080, 29901, 13, 9984, 263, 9508, 393, 12747, 565, 263, 15837, 8126, 278, 1667, 2969, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 5596, 565, 278, 15837, 8126, 278, 1667, 2969, 29889, 12027, 7420, 596, 6577, 29887]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"input_ids[:40]\", tmp[\"input_ids\"][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43cf4813-84a7-4f18-a925-182d3dd717e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: labels[:40]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5596, 565, 278, 15837, 8126, 278, 1667, 2969, 29889, 12027, 7420, 596, 6577, 29887]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "debug(\"labels[:40]\", tmp[\"labels\"][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f791bbb-8ac4-4d20-808e-e02c6559e441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed17b92a4fb4e0199ae852f43a2484f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290d4ee86aab4913b7ef2c6d6d7a24b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds_raw.map(preprocess, remove_columns=train_ds_raw.column_names)\n",
    "test_ds = test_ds_raw.map(preprocess, remove_columns=test_ds_raw.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf3c003-e6a6-4c75-ad2a-80367b32d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausalLMWithLabels:\n",
    "    tokenizer: AutoTokenizer\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "     \n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        for f in features:\n",
    "            f.pop(\"labels\")\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,          # pad to longest\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded_labels = []\n",
    "        for lab in labels:\n",
    "            pad_len = max_len - len(lab)\n",
    "            if pad_len > 0:\n",
    "                lab = lab + [self.label_pad_token_id] * pad_len\n",
    "            else:\n",
    "                lab = lab[:max_len]  # safety trim\n",
    "            padded_labels.append(lab)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9edd2cca-9c34-4dcb-93af-f61a46254bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be1fea2-870e-4e7e-b5d7-9595b5235f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d20d5839-66b3-44c5-809c-626f144ea27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e1c7b22-8b65-4301-975d-dfded4bdcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9304/1138148192.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCausalLMWithLabels(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea4b5149-4b19-4bd2-8c72-f6688dacc82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.735100</td>\n",
       "      <td>2.003084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.004400</td>\n",
       "      <td>1.601101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.288900</td>\n",
       "      <td>1.510775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=1.8340472550619216, metrics={'train_runtime': 16.9348, 'train_samples_per_second': 9.035, 'train_steps_per_second': 1.24, 'total_flos': 47467647135744.0, 'train_loss': 1.8340472550619216, 'epoch': 3.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f3c6a39-fdc4-42c5-89ef-3000fc27374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./prompt_lora_ckpt/tokenizer_config.json',\n",
       " './prompt_lora_ckpt/special_tokens_map.json',\n",
       " './prompt_lora_ckpt/chat_template.jinja',\n",
       " './prompt_lora_ckpt/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdcd50d9-18c8-4032-b13d-810f65b3308f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "vanilla_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af04f4d3-1f04-43ab-b466-ab5c8a8b7c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_base = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "trained_model = PeftModel.from_pretrained(trained_base, OUT_DIR)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4515c847-0564-4393-ac81-850dd2133cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_for_input(model, inp: str):\n",
    "    prompt = format_prompt(inp)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in decoded:\n",
    "        decoded = decoded.split(\"### Response:\")[-1].strip()\n",
    "    return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8184dfa8-a372-496b-bfb9-1ab321481564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test 1]\n",
      "INPUT: Make a prompt that extracts pros and cons.\n",
      "GOLD: Extract pros and cons. Make sure to output two bullet lists: Pros and Cons.\n",
      "VANILLA: Pros:\n",
      "- The app is user-friendly and easy to navigate.\n",
      "- The app provides a variety of features, including weather forecasts, news updates, and sports scores.\n",
      "- The app is available on both iOS and Android platforms, making it accessible to a wider audience.\n",
      "\n",
      "Cons:\n",
      "- The app may not be suitable for users who are not familiar with the\n",
      "TRAINED: Extract the pros and cons. Make sure to highlight them. Make sure to keep it short. Make sure to make it clear. Make sure to make it short. Make sure to make it clear. Make sure to make it short. Make sure to make it clear. Make sure to make it short. Make sure to make it clear. Make sure to make it short. Make sure to make\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 2]\n",
      "INPUT: Make a prompt that explains a Python function line by line.\n",
      "GOLD: Explain the Python code line by line. Make sure you say what each line does and why it matters.\n",
      "VANILLA: ```python\n",
      "def square(num):\n",
      "    \"\"\"\n",
      "    This function takes a single integer parameter 'num' and returns the square of num.\n",
      "    \"\"\"\n",
      "    return num ** 2\n",
      "\n",
      "# Example usage:\n",
      "print(square(5))  # Output: 25\n",
      "```\n",
      "TRAINED: Explain the function line by line. Make sure to include the function name and parameters. Make sure to use bullet points. Make sure to use short sentences. Make sure to use clear language. Make sure to proofread. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 3]\n",
      "INPUT: Make a prompt that rewrites English text in academic style.\n",
      "GOLD: Rewrite the text in academic style. Make sure it is clearer and more concise. Show only the revised version.\n",
      "VANILLA: Please provide a prompt that rewrites the text in academic style:\n",
      "\n",
      "1. Please provide a prompt that rewrites the text in academic style:\n",
      "\n",
      "2. Please provide a prompt that rewrites the text in academic style:\n",
      "\n",
      "3. Please provide a prompt that rewrites the text in academic style:\n",
      "\n",
      "4. Please provide a prompt that rewrites\n",
      "TRAINED: Rewrite the text in academic style. Make sure to keep the meaning. Make sure to use academic language. Make sure to proofread. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to proofread again. Make sure to\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 4]\n",
      "INPUT: Make a prompt that rewrites Korean instructions more clearly.\n",
      "GOLD: Rewrite the Korean instructions clearly. Make sure it sounds natural. Show only the final version.\n",
      "VANILLA: Please follow the Korean instructions below:\n",
      "\n",
      "1. 입력 포맷: 입력 포맷은 숫자 또는 문자열로 입력된다.\n",
      "\n",
      "2. �����\n",
      "TRAINED: Rewrite the instructions in simpler language. Make sure to keep the meaning. Make sure to use short sentences. Make sure to use simple words. Make sure to make it clear. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 5]\n",
      "INPUT: Make a prompt that extracts structured info from a messy paragraph.\n",
      "GOLD: Extract the info from the text. Make sure to output it as JSON. Keep the fields simple and clear.\n",
      "VANILLA: ```\n",
      "Please enter a messy paragraph:\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed euismod, nulla sit amet bibendum commodo, nulla nulla bibendum nunc, eu ultricies nulla nulla sit amet nisl. Sed euismod, nulla sit amet bibendum commodo,\n",
      "TRAINED: Extract the structured info. Make sure to keep it short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 6]\n",
      "INPUT: Make a prompt that summarizes an interview transcript.\n",
      "GOLD: Summarize the interview. Use 3 bullet points only. Make sure each bullet is under 20 words.\n",
      "VANILLA: Thank you for your time and interest in our company. Here is a summary of our interview with [Name], who is currently working as [Position] at [Company Name].\n",
      "\n",
      "[Name] is a [Age]-year-old [Gender] who has been with our company for [Number of Years] years. They have [Number of Job Titles] in their current role\n",
      "TRAINED: Summarize the interview. Make sure to keep it short. Make sure to use bullet points. Make sure to keep it under 100 words. Make sure to use short sentences. Make sure to keep it clear. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure to make it short. Make sure to make it short\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 7]\n",
      "INPUT: Make a prompt that converts example-label pairs into DPO preference pairs.\n",
      "GOLD: Convert the examples into DPO pairs. Explain the gold answer in one sentence. Make sure the rejected answer looks plausible but wrong.\n",
      "VANILLA: ```\n",
      "Please enter example-label pairs separated by commas:\n",
      "1,2\n",
      "3,4\n",
      "5,6\n",
      "7,8\n",
      "9,10\n",
      "11,12\n",
      "13,14\n",
      "15,16\n",
      "17,18\n",
      "19,20\n",
      "```\n",
      "\n",
      "Example-label pairs:\n",
      "1. \"apple\"\n",
      "TRAINED: Convert the pairs into DPO preference pairs. Make sure to keep the original order. Make sure to use the same label for each pair. Make sure to keep the order of the pairs. Make sure to keep the order of the preference pairs. Make sure to keep the order of the preference pairs. Make sure to keep the order of the preference pairs. Make sure to keep the order of the preference\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 8]\n",
      "INPUT: Make a prompt that summarizes a YouTube review.\n",
      "GOLD: Summarize the review in 3 bullet points. Make sure they are short and clear.\n",
      "VANILLA: \"I absolutely love this product! The design is sleek and modern, and the functionality is top-notch. The app is easy to use, and the customer support is top-notch. I highly recommend this product to anyone looking for a high-quality, user-friendly app for managing their social media accounts.\"\n",
      "TRAINED: Summarize the review. Make sure to keep it short. Make sure to use bullet points. Make sure to keep it under 100 words. Make sure to use short sentences. Make sure to keep it short. Make sure to keep it short. Make sure to keep it short. Make sure to keep it short. Make sure to keep it short. Make sure to keep it short\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 9]\n",
      "INPUT: Make a prompt that checks if a research question is clear.\n",
      "GOLD: Check if the question is clear. Explain why in one sentence. Rewrite it shortly.\n",
      "VANILLA: Sure, here's an example prompt:\n",
      "\n",
      "- Research question: What are the long-term effects of social media use on mental health?\n",
      "\n",
      "- Clear response: Yes, this research question is clear and specific.\n",
      "TRAINED: Check if the research question is clear. Explain it in 1 sentence. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is short. Make sure it is clear. Make sure it is\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Test 10]\n",
      "INPUT: Make a prompt that checks inconsistencies in a list of statements.\n",
      "GOLD: Check the statements for inconsistencies. Explain each issue shortly. Make sure to be direct.\n",
      "VANILLA: ```\n",
      "Enter a list of statements separated by commas:\n",
      "1. John likes to eat pizza\n",
      "2. Jane loves to cook\n",
      "3. John and Jane both like pizza\n",
      "4. Jane doesn't like pizza\n",
      "5. John and Jane both love pizza\n",
      "```\n",
      "\n",
      "The input list contains inconsistencies.\n",
      "TRAINED: Check for inconsistencies. Explain them. Make sure to fix them. Make sure to explain them. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them again. Explain them\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, ex in enumerate(test_ds_raw):\n",
    "    inp = ex[\"input\"]\n",
    "    gold = ex[\"output\"]\n",
    "\n",
    "    vanilla_pred = generate_for_input(vanilla_model, inp)\n",
    "    trained_pred = generate_for_input(trained_model, inp)\n",
    "\n",
    "    results.append({\n",
    "        \"input\": inp,\n",
    "        \"gold\": gold,\n",
    "        \"vanilla_pred\": vanilla_pred,\n",
    "        \"trained_pred\": trained_pred\n",
    "    })\n",
    "\n",
    "    print(f\"\\n[Test {i+1}]\")\n",
    "    print(\"INPUT:\", inp)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"VANILLA:\", vanilla_pred)\n",
    "    print(\"TRAINED:\", trained_pred)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a93cb10b-d140-4500-b29f-0256a96ff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer_R = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0c8192a-bc43-4b9e-a3b5-db39ff7b3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rouge_f1(pred, ref):\n",
    "    s = scorer.score(ref, pred)\n",
    "    return s[\"rouge1\"].fmeasure, s[\"rougeL\"].fmeasure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64ac1199-841a-4723-85c4-5bf3f7138240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "random_constant_prompt = random.choice(train_ds_raw)['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "791bb0ae-0ef0-4a15-b226-5785bc5cccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Randomly Selected Constant Prompt is: 'Find unclear parts of the definition. Explain them in one short sentence. Rewrite it clearly.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"DEBUG: Randomly Selected Constant Prompt is: '{random_constant_prompt}'\")\n",
    "\n",
    "rand_r1_scores = []\n",
    "rand_rl_scores = []\n",
    "\n",
    "# Calculate ROUGE for this static baseline using test_ds_raw\n",
    "for ex in test_ds_raw:\n",
    "    gold = ex[\"output\"]\n",
    "    pred = random_constant_prompt  # Static prediction (Randomly chosen once)\n",
    "    \n",
    "    r1, rl = rouge_f1(pred, gold)\n",
    "    rand_r1_scores.append(r1)\n",
    "    rand_rl_scores.append(rl)\n",
    "\n",
    "# ==========================================\n",
    "# 2. VANILLA & TRAINED CALCULATION\n",
    "# ==========================================\n",
    "\n",
    "van_r1s, van_rls = [], []\n",
    "trn_r1s, trn_rls = [], []\n",
    "\n",
    "# Calculate ROUGE for models from your existing 'results' list\n",
    "for r in results:\n",
    "    # Vanilla\n",
    "    vr1, vrl = rouge_f1(r[\"vanilla_pred\"], r[\"gold\"])\n",
    "    van_r1s.append(vr1)\n",
    "    van_rls.append(vrl)\n",
    "    \n",
    "    # Trained\n",
    "    tr1, trl = rouge_f1(r[\"trained_pred\"], r[\"gold\"])\n",
    "    trn_r1s.append(tr1)\n",
    "    trn_rls.append(trl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ec0ee0e-f369-49c0-b199-793bce21ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL ROUGE SCORE COMPARISON (ALL MODELS)\n",
      "============================================================\n",
      "Model                | ROUGE-1    | ROUGE-L   \n",
      "-----------------------------------------------------------\n",
      "Randomly Chosen Unified | 0.1958     | 0.1707\n",
      "Vanilla Model        | 0.1255     | 0.1115\n",
      "Trained Model        | 0.2089     | 0.2041\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ROUGE SCORE COMPARISON (ALL MODELS)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} | {'ROUGE-1':<10} | {'ROUGE-L':<10}\")\n",
    "print(\"-\" * 59)\n",
    "\n",
    "# Helper to print row\n",
    "def print_row(name, r1_scores, rl_scores):\n",
    "    avg_r1 = np.mean(r1_scores)\n",
    "    avg_rl = np.mean(rl_scores)\n",
    "    print(f\"{name:<20} | {avg_r1:.4f}     | {avg_rl:.4f}\")\n",
    "\n",
    "print_row(\"Randomly Chosen Unified\",  rand_r1_scores, rand_rl_scores)\n",
    "print_row(\"Vanilla Model\",    van_r1s,       van_rls)\n",
    "print_row(\"Trained Model\",    trn_r1s,       trn_rls)\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f33e6a48-050f-4883-997a-fdcbe91d1186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results to test_generations_with_rouge.jsonl...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"test_generations_with_rouge.jsonl\"\n",
    "print(f\"\\nSaving results to {filename}...\")\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, r in enumerate(results):\n",
    "        # Add Vanilla Scores\n",
    "        r[\"vanilla_rouge1\"] = van_r1s[i]\n",
    "        r[\"vanilla_rougeL\"] = van_rls[i]\n",
    "        \n",
    "        # Add Trained Scores\n",
    "        r[\"trained_rouge1\"] = trn_r1s[i]\n",
    "        r[\"trained_rougeL\"] = trn_rls[i]\n",
    "        \n",
    "        # Add Random Baseline Data & Scores\n",
    "        r[\"random_pred\"]   = random_constant_prompt\n",
    "        r[\"random_rouge1\"] = rand_r1_scores[i]\n",
    "        r[\"random_rougeL\"] = rand_rl_scores[i]\n",
    "        \n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"DONE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
