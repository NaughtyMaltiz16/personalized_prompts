{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a7908c-d419-4ea2-81c4-2c61def9dbae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.1\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.40.1)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.40.1)\n",
      "  Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.40.1)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (2024.7.4)\n",
      "Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.19.1 transformers-4.40.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting peft==0.10.0\n",
      "  Using cached peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.3.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.40.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.66.5)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.10.0)\n",
      "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (1.2.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (0.19.1)\n",
      "Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [peft]erate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 peft-0.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes==0.43.1\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1) (2.3.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1) (1.24.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
      "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets==2.19.0\n",
      "  Using cached datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (1.24.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets==2.19.0)\n",
      "  Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.19.0)\n",
      "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (4.66.5)\n",
      "Collecting xxhash (from datasets==2.19.0)\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.19.0)\n",
      "  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2024.2.0)\n",
      "Collecting aiohttp (from datasets==2.19.0)\n",
      "  Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.19.0)\n",
      "  Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->datasets==2.19.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.19.0) (3.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2024.7.4)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.19.0)\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\n",
      "Using cached datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [datasets]/14\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 async-timeout-5.0.1 datasets-2.19.0 dill-0.3.8 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.16 propcache-0.4.1 pyarrow-22.0.0 pyarrow-hotfix-0.7 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (5.27.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.40.1\n",
    "!pip install peft==0.10.0\n",
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install datasets==2.19.0\n",
    "!pip install accelerate\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530d64bd-7c95-4d2c-ad27-68fd4a4d2845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"###\") # I manually hid this after completing the experiments due to privacy issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb2d445-9fff-4751-90aa-25b9e8438c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes --quiet\n",
    "\n",
    "## restart kernel and continue below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48ca134-8971-49ae-a7eb-06a9c21da702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    teacher_model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    student_model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "    train_sub_size: int = 100\n",
    "    test_sub_size: int = 10\n",
    "\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 256\n",
    "\n",
    "    base_output_dir: str = \"./gsm8k_qlora_exps\"\n",
    "\n",
    "    num_train_epochs: int = 2\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    warmup_ratio: float = 0.03\n",
    "    logging_steps: int = 20\n",
    "\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: tuple = (\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    )\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc7f778-c8dc-4713-9dd7-582e28b71159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cot_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert math tutor.\\n\"\n",
    "        \"Solve the problem step by step, then clearly state the final numeric answer.\\n\"\n",
    "        \"At the very end, write the answer in the form: Answer: <number>.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Let's think step by step.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_answer_only_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful math problem solver.\\n\"\n",
    "        \"Solve the following question and give only the final numeric answer.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528db871-9bd0-473c-af9d-082ef22d49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0),\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if gen.startswith(prompt):\n",
    "        gen = gen[len(prompt):]\n",
    "    return gen.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ee1c38-cc19-4411-94c3-35feacaeec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the GSM8K (math) subset\n",
    "\n",
    "def load_gsm8k_subset(cfg: Config) -> DatasetDict:\n",
    "    ds = load_dataset(\"gsm8k\", \"main\")\n",
    "    train_full = ds[\"train\"]\n",
    "    test_full = ds[\"test\"]\n",
    "\n",
    "    train_shuffled = train_full.shuffle(seed=42)\n",
    "    train_sub = train_shuffled.select(range(cfg.train_sub_size))\n",
    "\n",
    "    test_shuffled = test_full.shuffle(seed=42)\n",
    "    test_sub = test_shuffled.select(range(cfg.test_sub_size))\n",
    "\n",
    "    def add_gold(ex):\n",
    "        ex[\"gold_final\"] = extract_final_answer_from_gsm8k(ex[\"answer\"])\n",
    "        return ex\n",
    "\n",
    "    train_sub = train_sub.map(add_gold)\n",
    "    test_sub = test_sub.map(add_gold)\n",
    "\n",
    "    return DatasetDict({\"train\": train_sub, \"test\": test_sub})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc0d976-e684-458d-a1f5-bcffebb7e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the model with Qlora\n",
    "\n",
    "def load_student_for_qlora(cfg: Config):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.student_model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_r,\n",
    "        lora_alpha=cfg.lora_alpha,\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=list(cfg.lora_target_modules),\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_teacher_model(cfg: Config):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.teacher_model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.teacher_model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84751117-b05a-4377-a669-6aa7898ad26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating the \"hard\" datapoints\n",
    "\n",
    "def compute_logprob_hardness(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_ds: Dataset,\n",
    "    cfg: Config,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    def encode_for_hardness(ex):\n",
    "        prompt = build_answer_only_prompt(ex[\"question\"])\n",
    "        target = ex[\"gold_final\"]\n",
    "        full_text = prompt + \" \" + target\n",
    "\n",
    "        enc = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=cfg.max_source_length + cfg.max_target_length,\n",
    "        )\n",
    "        enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_source_length,\n",
    "        )[\"input_ids\"]\n",
    "        enc[\"prompt_len\"] = len(prompt_ids)\n",
    "        return enc\n",
    "\n",
    "    encoded = train_ds.map(encode_for_hardness, remove_columns=train_ds.column_names)\n",
    "    encoded.set_format(type=\"torch\")\n",
    "\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(encoded, batch_size=cfg.per_device_eval_batch_size)\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        prompt_len = batch[\"prompt_len\"].to(model.device)  # [B]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits  # [B, T, V]\n",
    "\n",
    "        shift_logits = logits[:, :-1, :].contiguous()   # [B, T-1, V]\n",
    "        shift_labels = labels[:, 1:].contiguous()       # [B, T-1]\n",
    "\n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        target_tokens = shift_labels.unsqueeze(-1)\n",
    "        token_logp = log_probs.gather(-1, target_tokens).squeeze(-1)  # [B, T-1]\n",
    "\n",
    "        B, Tm1 = shift_labels.shape\n",
    "        positions = torch.arange(Tm1, device=model.device).unsqueeze(0).expand(B, -1)\n",
    "        mask = positions >= (prompt_len.unsqueeze(1) - 1)\n",
    "\n",
    "        token_logp = token_logp * mask\n",
    "        n_tokens = mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        hardness = -(token_logp.sum(dim=1) / n_tokens)  # [B]\n",
    "        all_scores.extend(hardness.detach().float().cpu().numpy().tolist())\n",
    "\n",
    "    return np.array(all_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10d6d74-3f83-43c9-9486-f248f822e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tokenizaiton\n",
    "\n",
    "def make_answer_only_dataset(\n",
    "    ds: Dataset,\n",
    "    tokenizer,\n",
    "    cfg: Config,\n",
    "    target_field: str = \"gold_final\",\n",
    ") -> Dataset:\n",
    "\n",
    "    def preprocess(ex):\n",
    "        question = ex[\"question\"]\n",
    "        target = ex[target_field]\n",
    "        prompt = build_answer_only_prompt(question)\n",
    "        full = prompt + \" \" + target\n",
    "\n",
    "        enc = tokenizer(\n",
    "            full,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_source_length + cfg.max_target_length,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_source_length,\n",
    "        )[\"input_ids\"]\n",
    "        prompt_len = len(prompt_ids)\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "\n",
    "    tokenized = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "    tokenized.set_format(\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def make_cot_dataset(\n",
    "    ds: Dataset,\n",
    "    tokenizer,\n",
    "    cfg: Config,\n",
    "    target_field: str,\n",
    ") -> Dataset:\n",
    "\n",
    "    def preprocess(ex):\n",
    "        q = ex[\"question\"]\n",
    "        target_text = ex[target_field]\n",
    "        prompt = build_cot_prompt(q)\n",
    "        full = prompt + \"\\n\" + target_text\n",
    "\n",
    "        enc = tokenizer(\n",
    "            full,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_source_length + cfg.max_target_length,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_source_length,\n",
    "        )[\"input_ids\"]\n",
    "        prompt_len = len(prompt_ids)\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "\n",
    "    tokenized = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "    tokenized.set_format(\"torch\")\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a782ad8c-0ba1-40c6-a406-156a1bd61c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "\n",
    "\n",
    "def train_student_qlora(\n",
    "    cfg: Config,\n",
    "    tokenizer,\n",
    "    train_dataset: Dataset,\n",
    "    run_name: str,\n",
    "    learning_rate=None,\n",
    "    num_epochs=None,\n",
    "):\n",
    "\n",
    "    model = load_student_for_qlora(cfg)\n",
    "    lr = learning_rate if learning_rate is not None else cfg.learning_rate\n",
    "    epochs = num_epochs if num_epochs is not None else cfg.num_train_epochs\n",
    "\n",
    "    output_dir = os.path.join(cfg.base_output_dir, run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        logging_steps=cfg.logging_steps,\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return trainer.model, output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ae4b08-8739-4ae5-a056-ce295ff3a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_gsm8k_accuracy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_ds: Dataset,\n",
    "    cfg: Config,\n",
    ") -> float:\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for ex in tqdm(test_ds, total=len(test_ds), desc=\"Evaluating GSM8K\"):\n",
    "        q = ex[\"question\"]\n",
    "        gold = ex[\"gold_final\"]\n",
    "\n",
    "        prompt = build_cot_prompt(q)\n",
    "        gen = generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=cfg.max_target_length,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        pred_ans = extract_final_number_from_text(gen)\n",
    "        if pred_ans == gold:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0535ef2c-6de6-4135-922b-e86c601a6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT generation\n",
    "\n",
    "def attach_teacher_cot(train_ds: Dataset, teacher_model, teacher_tokenizer, cfg: Config) -> Dataset:\n",
    "    def add_cot(ex):\n",
    "        q = ex[\"question\"]\n",
    "        prompt = build_cot_prompt(q)\n",
    "        gen = generate_text(\n",
    "            teacher_model,\n",
    "            teacher_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=cfg.max_target_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        ex[\"teacher_cot_answer\"] = gen\n",
    "        return ex\n",
    "\n",
    "    return train_ds.map(add_cot)\n",
    "\n",
    "\n",
    "def attach_student_cot(train_ds: Dataset, student_model, tokenizer, cfg: Config) -> Dataset:\n",
    "    def add_cot(ex):\n",
    "        q = ex[\"question\"]\n",
    "        prompt = build_cot_prompt(q)\n",
    "        gen = generate_text(\n",
    "            student_model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=cfg.max_target_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        ex[\"student_cot_answer\"] = gen\n",
    "        return ex\n",
    "\n",
    "    return train_ds.map(add_cot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f4198e-2217-40a0-be6c-2a63c10c67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WE are done with the functions,\n",
    "## now we execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a513eaa-4764-41a1-96e3-c00877b0a019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd8d4c7b1b84447a75b28d543e9f793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7b3c855b8b409c876778ede4dee637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8f0dafa03d4b9daca2811dfc26a149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef8491962cf4426b0bea62fa2d9680d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed5882bcf9f4048ae529d6d8cd3bfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a275ff69ff241bb9ceaf711ac12027c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8366a86aee84406fbed9ae6961f2f34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 100\n",
      "Test size: 10\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K dataset subset\n",
    "\n",
    "ds = load_gsm8k_subset(cfg)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f1dc259-91e2-47df-9bbd-17b0ea11154b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b20594087342f691e2d0e18640ef02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ff873b1a4c4ea5a2efe939f8f77ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e58c66c9b9a47329badd2b7ac17a94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c28f2fb54847f7be35bda5e338f114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b721c086a0cd4098aa408c8e870e9c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f249f2d969b49dcbd6f6c0574c03760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03705de5bcc64263926299a2131d184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer + Teacher model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.student_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "teacher_model, teacher_tokenizer = load_teacher_model(cfg)\n",
    "\n",
    "teacher_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca29cfe4-40f4-4521-a56b-b0e3b7540c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21878db78e724544bf58a28f8d8a3b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d8c0c5029c4961b31576893f58cfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64370bd79e4c490b9f63ea4635b55583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f2f042285246f0a6883cd506c8bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e85fe091cc4e2292eb615760ecc136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c4261553e9496e8217061a625c6b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c743719f8f644f8a3aafdcf1f448539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4234a5351bd24b48aeca0b9a475c0d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.367900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter to: ./gsm8k_qlora_exps/student_answer_only_qlora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gsm8k_qlora_exps/student_answer_only_qlora/tokenizer_config.json',\n",
       " './gsm8k_qlora_exps/student_answer_only_qlora/special_tokens_map.json',\n",
       " './gsm8k_qlora_exps/student_answer_only_qlora/chat_template.jinja',\n",
       " './gsm8k_qlora_exps/student_answer_only_qlora/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Train baseline \n",
    "\n",
    "answer_only_train_tokenized = make_answer_only_dataset(\n",
    "    train_ds,\n",
    "    tokenizer,\n",
    "    cfg,\n",
    "    target_field=\"gold_final\",\n",
    ")\n",
    "\n",
    "if \"labels\" in answer_only_train_tokenized.column_names:\n",
    "    answer_only_train_tokenized = answer_only_train_tokenized.remove_columns(\"labels\")\n",
    "    \n",
    "answer_student, answer_ckpt_dir = train_student_qlora(\n",
    "    cfg,\n",
    "    tokenizer,\n",
    "    answer_only_train_tokenized,\n",
    "    run_name=\"student_answer_only_qlora\",\n",
    ")\n",
    "\n",
    "save_dir = os.path.join(cfg.base_output_dir, \"student_answer_only_qlora\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving LoRA adapter to: {save_dir}\")\n",
    "answer_student.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edbadd94-97c9-41e8-9f91-106dcbfc0c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27316765d4fe490cb355518bfd368118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating GSM8K:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer-only: 0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_answer = evaluate_gsm8k_accuracy(answer_student, tokenizer, test_ds, cfg)\n",
    "print(\"Baseline Answer-only:\", acc_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5993ab60-49f0-4be7-b9df-37fc52e53d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models: ['__', 'teacher_model', '_15', 'answer_student']\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, nn.Module):  # detect PyTorch model\n",
    "        to_delete.append(name)\n",
    "\n",
    "for name in to_delete:\n",
    "    del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Deleted models:\", to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59d11e6-c32d-46e4-9d10-c5daa91a3627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708dc53cbaff4457837b5f5d77aee363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "answer_student = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,               \n",
    "    torch_dtype=torch.bfloat16,  \n",
    ")\n",
    "answer_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "791aadc2-9bb9-48c2-8fb0-a818ad87598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard: 52\n",
      "Easy: 48\n"
     ]
    }
   ],
   "source": [
    "#  Compute hardness (log-prob)\n",
    "\n",
    "hardness_scores = compute_logprob_hardness(answer_student, tokenizer, train_ds, cfg)\n",
    "\n",
    "hard_fraction = 0.5\n",
    "threshold = np.quantile(hardness_scores, 1.0 - hard_fraction)\n",
    "\n",
    "hard_indices = set(np.where(hardness_scores >= threshold)[0])\n",
    "easy_indices = set(np.where(hardness_scores < threshold)[0])\n",
    "\n",
    "print(\"Hard:\", len(hard_indices))\n",
    "print(\"Easy:\", len(easy_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75e11bb2-ee58-44c5-b9c1-5751c441eb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8c6ae59a347d2b1301805ba62e49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "answer_student = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,               \n",
    "    torch_dtype=torch.bfloat16,  \n",
    ")\n",
    "answer_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2213ed9-0986-4e40-99b0-d1a89aeb5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a433af51b624a62a8ec9c23f53c5367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9290475a9c8448bbb6ffe01adcc6860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       " 'answer': 'Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       " 'gold_final': '16',\n",
       " 'teacher_cot_answer': 'Step 1: Mimi picked up 2 dozen seashells.  A dozen is 12.  So, 2 dozen is 24 seashells.\\nStep 2: Kyle found twice as many shells as Mimi.  Mimi had 24 shells.  So, Kyle found 2 x 24 = 48 shells.\\nStep 3: Leigh grabbed one-third of the shells that Kyle found.  Kyle found 48 shells.  One-third is 48 / 3 = 16.\\n\\nAnswer: 16.  Answer: <16>.  The answer is in the correct format.  I am done.  The final answer is 16.  I am finished.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Attach teacher CoT answers\n",
    "teacher_model, teacher_tokenizer = load_teacher_model(cfg)\n",
    "train_with_teacher_cot = attach_teacher_cot(\n",
    "    train_ds, teacher_model, teacher_tokenizer, cfg\n",
    ")\n",
    "\n",
    "print(len(train_with_teacher_cot))\n",
    "train_with_teacher_cot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ceab84c-b499-4bfb-b256-ba809bae6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58829459eb494d728f547a42f5825db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       " 'answer': 'Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       " 'gold_final': '16',\n",
       " 'teacher_cot_answer': 'Step 1: Mimi picked up 2 dozen seashells.  A dozen is 12.  So, 2 dozen is 24 seashells.\\nStep 2: Kyle found twice as many shells as Mimi.  Mimi had 24 shells.  So, Kyle found 2 x 24 = 48 shells.\\nStep 3: Leigh grabbed one-third of the shells that Kyle found.  Kyle found 48 shells.  One-third is 48 / 3 = 16.\\n\\nAnswer: 16.  Answer: <16>.  The answer is in the correct format.  I am done.  The final answer is 16.  I am finished.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye',\n",
       " 'student_cot_answer': 'Step 1: Calculate how many seashells Mimi picked up.\\nAnswer: 24\\n\\nStep 2: Calculate how many seashells Kyle found.\\nAnswer: 48\\n\\nStep 3: Calculate how many seashells Leigh grabbed.\\nAnswer: 16\\n\\nStep 4: Determine the final answer.\\nAnswer: 16\\n\\nAnswer: 16'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Attach student CoT answers\n",
    "\n",
    "train_with_both_cot = attach_student_cot(\n",
    "    train_with_teacher_cot, answer_student, tokenizer, cfg\n",
    ")\n",
    "\n",
    "print(len(train_with_both_cot))\n",
    "train_with_both_cot[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94b06516-c4fe-44ef-8e5b-6b60a08607b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aa88467ba548ea8ffd98a4dd3e4e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()   \n",
    "###### 1: Self-CoT dataset (BASELINE)\n",
    "\n",
    "self_cot_train_tokenized = make_cot_dataset(\n",
    "    train_with_both_cot,\n",
    "    tokenizer,\n",
    "    cfg,\n",
    "    target_field=\"student_cot_answer\",\n",
    ")\n",
    "\n",
    "\n",
    "if \"labels\" in self_cot_train_tokenized.column_names:\n",
    "    self_cot_train_tokenized = self_cot_train_tokenized.remove_columns(\"labels\")\n",
    "    \n",
    "self_cot_train_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3dd8edc-ba7f-4962-915f-3bbac8d26e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models: ['answer_student', '_19', 'teacher_model', '_25']\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, nn.Module):  # detect PyTorch model\n",
    "        to_delete.append(name)\n",
    "\n",
    "for name in to_delete:\n",
    "    del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Deleted models:\", to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc545593-bfc8-47d4-976d-f5485edcb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bcba75807140fc9930d9776d8d05ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Self-CoT QLoRA model to: ./gsm8k_qlora_exps/self_cot_student\n"
     ]
    }
   ],
   "source": [
    "\n",
    "self_cot_student, self_cot_ckpt = train_student_qlora(\n",
    "    cfg,\n",
    "    tokenizer,\n",
    "    self_cot_train_tokenized,\n",
    "    run_name=\"student_self_cot_qlora\",\n",
    ")\n",
    "\n",
    "save_dir = os.path.join(cfg.base_output_dir, \"self_cot_student\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving Self-CoT QLoRA model to: {save_dir}\")\n",
    "\n",
    "self_cot_student.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "torch.cuda.empty_cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6ae51b5-637a-468e-ab31-bd6b4396b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models: ['self_cot_student']\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, nn.Module):  # detect PyTorch model\n",
    "        to_delete.append(name)\n",
    "\n",
    "for name in to_delete:\n",
    "    del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Deleted models:\", to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a4a2151-3ffa-4efe-9e35-e8a4674a9679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6501fe8871c647dca13f42a133f0f074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.559300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Self-CoT QLoRA model to: ./gsm8k_qlora_exps/teacher_cot_student\n"
     ]
    }
   ],
   "source": [
    "###### 2: Full teacher-CoT dataset (BASELINE)\n",
    "\n",
    "teacher_cot_train_tokenized = make_cot_dataset(\n",
    "    train_with_both_cot,\n",
    "    tokenizer,\n",
    "    cfg,\n",
    "    target_field=\"teacher_cot_answer\",\n",
    ")\n",
    "\n",
    "if \"labels\" in teacher_cot_train_tokenized.column_names:\n",
    "    teacher_cot_train_tokenized = teacher_cot_train_tokenized.remove_columns(\"labels\")\n",
    "    \n",
    "teacher_cot_train_tokenized\n",
    "\n",
    "teacher_cot_student, teacher_cot_ckpt = train_student_qlora(\n",
    "    cfg,\n",
    "    tokenizer,\n",
    "    teacher_cot_train_tokenized,\n",
    "    run_name=\"student_teacher_full_cot_qlora\",\n",
    ")\n",
    "\n",
    "\n",
    "save_dir = os.path.join(cfg.base_output_dir, \"teacher_cot_student\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving Self-CoT QLoRA model to: {save_dir}\")\n",
    "\n",
    "teacher_cot_student.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "torch.cuda.empty_cache()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dc784f2-308e-43f0-95a7-360411b5c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models: ['base_three', 'selective_student']\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, nn.Module):  # detect PyTorch model\n",
    "        to_delete.append(name)\n",
    "\n",
    "for name in to_delete:\n",
    "    del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Deleted models:\", to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60500ce7-1970-4864-bd9c-6675759bee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9861427de45c7b1901b4588610c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### 3: Selective teacher-CoT dataset (MY PIPELINE)\n",
    "\n",
    "def build_selective_example(ex, idx):\n",
    "    ex = dict(ex)\n",
    "    if idx in hard_indices:\n",
    "        ex[\"selective_target\"] = ex[\"teacher_cot_answer\"]\n",
    "    else:\n",
    "        ex[\"selective_target\"] = ex[\"gold_final\"]\n",
    "    return ex\n",
    "\n",
    "selective_records = [\n",
    "    build_selective_example(train_with_both_cot[i], i)\n",
    "    for i in range(len(train_with_both_cot))\n",
    "]\n",
    "selective_train_ds = Dataset.from_list(selective_records)\n",
    "\n",
    "selective_train_tokenized = make_cot_dataset(\n",
    "    selective_train_ds,\n",
    "    tokenizer,\n",
    "    cfg,\n",
    "    target_field=\"selective_target\",\n",
    ")\n",
    "\n",
    "\n",
    "if \"labels\" in selective_train_tokenized.column_names:\n",
    "    selective_train_tokenized = selective_train_tokenized.remove_columns(\"labels\")\n",
    "selective_train_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "144740ad-c537-40a8-8f4b-d669cdf285ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66bf37eb09b4e158aa3b14a93c43acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:41, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.520600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Self-CoT QLoRA model to: ./gsm8k_qlora_exps/selective_student\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selective_student, selective_ckpt = train_student_qlora(\n",
    "    cfg,\n",
    "    tokenizer,\n",
    "    selective_train_tokenized,\n",
    "    run_name=\"student_teacher_selective_cot_qlora\",\n",
    "    num_epochs=4\n",
    ")\n",
    "\n",
    "save_dir = os.path.join(cfg.base_output_dir, \"selective_student\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving Self-CoT QLoRA model to: {save_dir}\")\n",
    "\n",
    "selective_student.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "torch.cuda.empty_cache()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e79fb2e3-1292-44f5-a37b-148e8dff3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models: ['selective_student']\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, nn.Module):  # detect PyTorch model\n",
    "        to_delete.append(name)\n",
    "\n",
    "for name in to_delete:\n",
    "    del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Deleted models:\", to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67beda76-7758-490d-8e98-88d081514eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19942586-93a0-46ea-8c1e-fbe25990833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79107516-1e14-4c53-81b0-eb79011e7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    student_model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    test_sub_size: int = 10\n",
    "    max_target_length: int = 256\n",
    "    seed: int = 42\n",
    "\n",
    "BASE_DIR = \"./gsm8k_qlora_exps\"\n",
    "DIR_SELF = os.path.join(BASE_DIR, \"self_cot_student\")\n",
    "DIR_TEACHER_FULL = os.path.join(BASE_DIR, \"teacher_cot_student\")\n",
    "DIR_SELECTIVE = os.path.join(BASE_DIR, \"selective_student\")\n",
    "OUT_JSON = os.path.join(BASE_DIR, \"test_eval_all_models.json\")\n",
    "\n",
    "cfg = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7d3853-111a-4dc5-8599-a18233b16c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_RE = re.compile(r\"[-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?\") \n",
    "def extract_final_answer_from_gsm8k(answer_str: str) -> str:\n",
    "    if \"####\" in answer_str:\n",
    "        return answer_str.split(\"####\")[-1].strip()\n",
    "    nums = _NUM_RE.findall(answer_str)\n",
    "    return nums[-1] if nums else answer_str.strip()\n",
    "\n",
    "def extract_final_number_from_text(text: str) -> str:\n",
    "\n",
    "    for line in reversed(text.splitlines()):\n",
    "        if \"answer\" in line.lower():\n",
    "            nums = _NUM_RE.findall(line)\n",
    "            if nums:\n",
    "                return nums[-1]\n",
    "    nums = _NUM_RE.findall(text)\n",
    "    return nums[-1] if nums else \"\"\n",
    "\n",
    "def normalize_number_str(s: str) -> str:\n",
    "\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "    s = s.strip()\n",
    "    s = s.rstrip(\".\")\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        x = float(s)\n",
    "        if abs(x - round(x)) < 1e-9:\n",
    "            return str(int(round(x)))\n",
    "        t = f\"{x:.10f}\".rstrip(\"0\").rstrip(\".\")\n",
    "        return t\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def is_correct_graceful(pred: str, gold: str) -> bool:\n",
    "    return normalize_number_str(pred) == normalize_number_str(gold)\n",
    "\n",
    "def build_cot_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert math tutor.\\n\"\n",
    "        \"Solve the problem step by step, then clearly state the final numeric answer.\\n\"\n",
    "        \"At the very end, write the answer in the form: Answer: <number>.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Let's think step by step.\\n\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if full.startswith(prompt):\n",
    "        full = full[len(prompt):]\n",
    "    return full.strip()\n",
    "\n",
    "def load_test_subset(cfg: Config):\n",
    "    test_full = load_dataset(\"gsm8k\", \"main\")[\"test\"]\n",
    "    test_sub = test_full.shuffle(seed=cfg.seed).select(range(cfg.test_sub_size))\n",
    "\n",
    "    def add_gold(ex):\n",
    "        ex[\"gold_final\"] = extract_final_answer_from_gsm8k(ex[\"answer\"])\n",
    "        return ex\n",
    "\n",
    "    return test_sub.map(add_gold)\n",
    "\n",
    "def load_peft_model(adapter_dir: str, base_model_name: str):\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\")\n",
    "    model = PeftModel.from_pretrained(base, adapter_dir).to(\"cuda\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model_on_test(model, tokenizer, test_ds, cfg: Config):\n",
    "    rows = []\n",
    "    correct = 0\n",
    "\n",
    "    for i, ex in enumerate(test_ds):\n",
    "        q = ex[\"question\"]\n",
    "        gold = ex[\"gold_final\"]\n",
    "        prompt = build_cot_prompt(q)\n",
    "\n",
    "        gen = generate_text(model, tokenizer, prompt, max_new_tokens=cfg.max_target_length)\n",
    "        pred = extract_final_number_from_text(gen)\n",
    "\n",
    "        ok = is_correct_graceful(pred, gold)\n",
    "        if ok:\n",
    "            correct += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"idx\": i,\n",
    "            \"question\": q,\n",
    "            \"gold_final_raw\": gold,\n",
    "            \"pred_final_raw\": pred,\n",
    "            \"gold_final_norm\": normalize_number_str(gold),\n",
    "            \"pred_final_norm\": normalize_number_str(pred),\n",
    "            \"is_correct\": ok,\n",
    "            \"generation\": gen,\n",
    "        })\n",
    "\n",
    "    acc = correct / len(test_ds)\n",
    "    return acc, rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165dafb7-d0d6-45cc-aa4b-2491f13b8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    test_ds = load_test_subset(cfg)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DIR_SELECTIVE)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    results = {\n",
    "        \"base_dir\": BASE_DIR,\n",
    "        \"student_model_name\": cfg.student_model_name,\n",
    "        \"test_sub_size\": cfg.test_sub_size,\n",
    "        \"seed\": cfg.seed,\n",
    "        \"comparison_rule\": \"graceful_numeric_match: normalize(pred_final) == normalize(gold_final)\",\n",
    "        \"models\": {}\n",
    "    }\n",
    "\n",
    "    # ---- SELF CoT ----\n",
    "    print(\"Loading SELF adapter:\", DIR_SELF)\n",
    "    m_self = load_peft_model(DIR_SELF, cfg.student_model_name)\n",
    "    acc_self, rows_self = evaluate_model_on_test(m_self, tokenizer, test_ds, cfg)\n",
    "    results[\"models\"][\"self_cot\"] = {\n",
    "        \"adapter_dir\": DIR_SELF,\n",
    "        \"accuracy\": acc_self,\n",
    "        \"per_example\": rows_self,\n",
    "    }\n",
    "    del m_self\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ---- TEACHER FULL CoT ----\n",
    "    print(\"Loading TEACHER-FULL adapter:\", DIR_TEACHER_FULL)\n",
    "    m_teacher = load_peft_model(DIR_TEACHER_FULL, cfg.student_model_name)\n",
    "    acc_teacher, rows_teacher = evaluate_model_on_test(m_teacher, tokenizer, test_ds, cfg)\n",
    "    results[\"models\"][\"teacher_full_cot\"] = {\n",
    "        \"adapter_dir\": DIR_TEACHER_FULL,\n",
    "        \"accuracy\": acc_teacher,\n",
    "        \"per_example\": rows_teacher,\n",
    "    }\n",
    "    del m_teacher\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ---- SELECTIVE CoT ----\n",
    "    print(\"Loading SELECTIVE adapter:\", DIR_SELECTIVE)\n",
    "    m_sel = load_peft_model(DIR_SELECTIVE, cfg.student_model_name)\n",
    "    acc_sel, rows_sel = evaluate_model_on_test(m_sel, tokenizer, test_ds, cfg)\n",
    "    results[\"models\"][\"selective_cot\"] = {\n",
    "        \"adapter_dir\": DIR_SELECTIVE,\n",
    "        \"accuracy\": acc_sel,\n",
    "        \"per_example\": rows_sel,\n",
    "    }\n",
    "    del m_sel\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ---- save ----\n",
    "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Saved all eval results to:\", OUT_JSON)\n",
    "    print(\"Accuracies:\", {\n",
    "        \"self_cot\": acc_self,\n",
    "        \"teacher_full_cot\": acc_teacher,\n",
    "        \"selective_cot\": acc_sel,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904ef346-fe5a-4336-a932-e24db1850618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -U peft transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffe22b-ae05-4892-9c90-30d97c245ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SELF adapter: ./gsm8k_qlora_exps/self_cot_student\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103693aea0cb48acb19f6f3872337799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TEACHER-FULL adapter: ./gsm8k_qlora_exps/teacher_cot_student\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75a3adf616e426497382e5bf06ed296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SELECTIVE adapter: ./gsm8k_qlora_exps/selective_student\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64325e-bf96-4807-8d84-7779b5618296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
